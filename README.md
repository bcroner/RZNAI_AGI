<h1 align="center">
System and Method for an AI Agent with Reasoning Capabilities
</h1>

Background of the Invention

**\[0001\]** 

Abstract

We introduce a new form of artificial intelligence that is capable of carrying out reasoning processes, making it possible for an AI agent to mentally investigate scenarios and form a plan of action in order to both achieve the Achieve State(s) (“correct decision(s)”) and avoid the Avoid State(s) (“incorrect decision(s)”).

We Claim

1. The construction of graph analysis algorithm instances (we use breadth-first search in the provided source code) from the Knowledge Bank and the solving for starting at the current state C and arriving at each of the Achieve State(s) A and Avoid State(s) A’.  
2. The increase of magnitude by a preset amount of the weight of a connection (synapse) between an input signal or an artificial neuron from one layer to the next upon any motivational reward(s) received and the decrease of the magnitude by a preset amount of the weight of a connection (synapse) between an input signal or an artificial neuron from one layer to the next upon any motivational disincentive(s) received.  
3. The re-targetting of an artificial neuron from a given layer when it fires an output to an artificial neuron’s input on the subsequent layer and the weight of the firing artificial neuron reaches a preset limit (we use 0 for this level in the provided reference source code).  
4. The retrieval of an unhashed unit of a former input from the knowledge bank upon the triggering of a recall operation by the setting of the recall bit in the output unit.

Specification

**\[0002\]** We receive input (the Input Unit) from either recall (from the knowledge bank) or from one or more sensors. The Input Unit contains a Read From Recall bit as well as zero or more bits that indicate which sensor the input data is sourced from. Recalling an Input State from the Knowledge Bank also brings in these zero or more sensor indicator bits as they are an integral part of the Input State data unit.

**\[0003\]** We discuss a modified artificial neural network approach. We introduce a neural network composed entirely of signed integers. We compose successive layers of integral artificial neurons which are the analog of the hidden layers used in floating point approaches. The first layer receives the input and the last layer sends output bit signals to the Output Unit. An artificial neuron is composed of a series of signed integer weights that refer to the input bit signals leading from it to the next layer as well as indicators for which artificial neurons in the next layer are addressed by the output bit signals. We select a number of these integral artificial neurons for each of the layers and a number of hidden layers in between. A portion (half in the provided reference source code) of the integral artificial neurons of a given layer sends negative integer values to the next layer and another portion (again, we use half in the provided reference source code) sends positive integer values to the next layer. The threshold for an integral artificial neuron to “fire” (meaning send its outputs to the next layer) is a preset limit (we use 0 or higher in the provided reference source code). To test whether a firing of a given artificial neuron occurs, we take the sum of all the integers from all the previous layer’s firing (or in the case of the first layer, from the Input Unit).

**\[0004\]** If an Achieve State is reached and rewards are available, increase all positive firing neuronal weights by a preset value (we use 1 in the provided source code) and decrease all negative firing neuronal weights (meaning further from zero on the negative side) by our preset value (again, we use 1 in the provided source code). If an Avoid State is reached and disincentives are available, decrement all positive firing neuronal weights by one and decrement all firing weights by one (meaning one closer to zero on the negative side). If an Achieve State is reached and rewards are not available, remove said Achieve State from the data structure that contains Achieve States. If an Avoid State is reached and disincentives are not available, remove said Avoid State from the data structure that contains Avoid States.

**\[0005\]** Every time a preset number of cycles of the system is performed (we use 1048576 in the provided source code), we reduce the magnitudes of all weights of the Integral Artificial Neurons by either subtracting a preset value from all positive-valued weights and adding said preset value to all negative-valued weights (meaning bringing them all closer to zero by said preset limit) or by shifting the bits of all our integer weights to the right by a preset number (we use 1 as this number so as to reduce the magnitudes of all weights by one-half).

**\[0006\]** When an integral artificial neuron weight reaches a preset limit (we use 0 in the provided source code), we “rewire” the artificial neuron to target a different artificial neuron of the next layer in the manner of your choice, whether round-robin, random, or some other method. We select a value for the weight for this new connection that is not equal to the preset limit that triggers our re-targeting (in the provided source code, it’s a nonzero value). Observe that for this to occur, our artificial neurons wire to the next layer sparsely, meaning that not all neurons from a layer are addressed by any neuron of the previous layer. We also take care to avoid having a neuron from one layer to target the same neuron of the next layer multiple times.

**\[0007\]** Once we have obtained our Current Input, we conjoin this to the end of the Input Queue and dequeue the oldest input from the beginning of the Input Queue. This is our short-term memory. We now seek to obtain an Output Unit. The Output Unit consists of an Action Sequence and a Parameter Sequence. After the Output Unit is obtained and any actions are performed, we address motivation with the Achieve State being recorded if a reward is available and with the Avoid State being recorded if a disincentive is available.

**\[0008\]** The Knowledge Bank format appears as follows, with the bits representing a given Action Sequence conjoined to the given Parameter Sequence to form a composite unit:  
Input State A \- Action Sequence 0 \-\> Location (Input State) X  
Input State A \- Action Sequence 1 \-\> Location (Input State) Y  
Input State A \- Action Sequence 2 \-\> Location (Input State) Z  
Locations X, Y, and Z contain the inputs reached when Action Sequences 0, 1, and 2 were performed after Input State A, respectively.

**\[0009\]** This is a logical implication in the form of (JK-\>P) && (JL-\>Q) && (JM-\>R), which can be represented by a directed graph on which we perform breadth-first search, a linear-time operation. We repeat for all Achieve State(s) in the system and we are left with action/step pairs. We repeat for all Avoid State(s) in the system and we are again left with action/step pairs.

**\[0010\]** To select the most appropriate action, we find action matches between the Achieve action/step pairs and the Avoid action/step pairs where the number of steps in the Avoid action/step pair is less than or equal to the number of steps in the matching Achieve action/step pairs. We eliminate these actions as options. We also eliminate all other Avoid actions from action/step pairs that don’t match any actions from the Achieve action/step pairs. If two or more Achieve States remain and are available from the current state, the one with the fewest intermediate states is selected. If there are multiple sequences leading to an Achieve State that are all tied for the fewest intermediate states, one is chosen at random. If no Achieve State action/step pairs are available, an action to perform is chosen at random from the remaining actions that have not been eliminated as possibilities. If no actions remain available, we select the action/step pair that results in the greatest number of steps before an Avoid State is reached. The output from this breadth-first search portion of the cycle is the Output Unit, which consists of the Output Recall Bit, the Action Sequence, and the Parameter Sequence.

**\[0011\]** One of the output bits is a recall bit where the AGI reads from its knowledge bank based on the portion of the output unit contained in the rest of the output unit (the Utility Sequence). For the AGI to perform tasks such as controlling robotics or outputting to a display terminal, there will be a portion of the Output Unit that contains the Utility Sequence. This Utility Sequence is then parsed so that the remainder of the Output Unit received from the 2CNF logic solver- the Parameter Sequence- is sent to the correct Output Target as identified by the Action Sequence from the solution of the 2CNF instance from the prior step of the cycle. The recall action accepts the Action Sequence and the Parameter Sequence from the Output Unit and uses that to address a location in the Knowledge Bank. The data from this location is fetched and sent into the input as the new Current Input. It is worth noting that the Knowledge Bank is updated with every single cycle with a location within the Knowledge Bank vectoring with an action to another location, possibly a new location, to reflect a dynamic world. If the recall bit is not set in the Output Unit, then the Parameter Sequence is sent to the Output Target addressed by the Action Sequence, and the action is performed according to the Parameter Sequence, and input comes from the current sensory source to begin the next cycle.

References

