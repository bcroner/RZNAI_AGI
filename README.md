<h1 align="center">
System and Method for an AI Agent with Reasoning Capabilities
</h1>

Abstract:

We introduce a new form of artificial intelligence that is capable of carrying out reasoning processes, making it possible for an AI agent to mentally investigate scenarios and form a plan of action in order to both achieve the Achieve State(s) (“correct decision(s)”) and avoid the Avoid State(s) (“incorrect decision(s)”).

Claims:

1. We give the AGI agent the ability to read from the Knowledge Bank by allocating a bit in the Output Unit we call the Output Recall Bit which triggers a lookup of a specific consequent recalled Unit (an Input Unit) that is mapped to by a (state, action) pair and we also allocate a bit in the Input Unit to indicate whether the incoming Input Unit is a result of the recall operation.  
2. To read from recall we construct boolean 2-satisfiability or 3-satisfiability instances from the Knowledge Bank and solve for arriving at each of the Achieve State(s) A and Avoid State(s) A’ by encoding one clause per Achieve State 2-SAT instance that forces the current state C to be true in the form (C | F), each of the Achieve States A to be true in the form (A | F), and each of the Avoid States A’ to be true in the form (A’ | F), where F is the false truth value, or with 3-SAT padding we pad with two false values F in a clause containing each combination of (C, X), where X is all of the Achieve States A or all of the Avoid States A’.  
3. We re-target an artificial neuron from a given layer when it fires an output to an artificial neuron’s input on the subsequent layer and the weight of the firing artificial neuron reaches a pre-set limit (we use 0 for this level in the provided reference source code).

Specification:

**\[0001\]** We receive input (the Input Unit) from either recall (from the knowledge bank) or from one or more sensors. The Input Unit contains a Read From Recall bit as well as zero or more bits that indicate which sensor the input data is sourced from. Recalling an Input State from the Knowledge Bank also brings in these zero or more sensor indicator bits as they are an integral part of the Input State data unit.

**\[0002\]** We discuss a modified artificial neural network approach. We introduce a neural network composed entirely of signed integers. We compose successive layers of integral artificial neurons which are the analog of the hidden layers used in floating point approaches. The first layer receives the input and the last layer sends output bit signals to the Output Unit. An artificial neuron is composed of a series of signed integer weights that refer to the input bit signals leading from it to the next layer as well as indicators for which artificial neurons in the next layer are addressed by the output bit signals. We select a number of these integral artificial neurons for each of the layers and a number of hidden layers in between. A portion (half in the provided reference source code) of the integral artificial neurons of a given layer sends negative integer values to the next layer and another portion (again, we use half in the provided reference source code) sends positive integer values to the next layer. The threshold for an integral artificial neuron to “fire” (meaning send its outputs to the next layer) is a pre-set limit (we use 0 or higher in the provided reference source code). To test whether a firing of a given artificial neuron occurs, we take the sum of all the integers from all the previous layer’s firing (or in the case of the first layer, from the Input Unit).

**\[0003\]** We keep track of all firings of all artificial neurons in a boolean array for each layer of these neurons. Every cycle we perform maintenance on the weights associated with the firing neurons by increasing the magnitudes of the weights of all connections between two firing neurons of two successive layers by a pre-set limit (we use 1 in the provided source code), meaning increase positive integer weights by this pre-set limit and decreasing negative integer weights by this pre-set limit (meaning bring them further out away from zero on the negative side).

**\[0004\]** If an Achieve State is reached and rewards are available, increase all positive firing neuronal weights by a pre-set value (we use 1 in the provided source code) and decrease all negative firing neuronal weights (meaning further from zero on the negative side) by our pre-set value (again, we use 1 in the provided source code). If an Avoid State is reached and disincentives are available, decrement all positive firing neuronal weights by one and decrement all firing weights by one (meaning one closer to zero on the negative side). If an Achieve State is reached and rewards are not available, remove said Achieve State from the data structure that contains Achieve States. If an Avoid State is reached and disincentives are not available, remove said Avoid State from the data structure that contains Avoid States.

**\[0005\]** Every time a pre-set number of cycles of the system is performed (we use 65536 in the provided source code), we reduce the magnitudes of all weights of the Integral Artificial Neurons by either subtracting a pre-set value from all positive-valued weights and adding said pre-set value to all negative-valued weights (meaning bringing them all closer to zero by said pre-set limit) or by shifting the bits of all our integer weights to the right by a pre-set number (we use 1 as this number so as to reduce the magnitudes of all weights by one-half).

**\[0006\]** When an integral artificial neuron weight reaches a pre-set limit (we use 0 in the provided source code), we “rewire” the artificial neuron to target a different artificial neuron of the next layer in the manner of your choice, whether round-robin, random, or some other method. We select a value for the weight for this new connection that is not equal to the pre-set limit that triggers our re-targeting (in the provided source code, it’s a nonzero value). Observe that for this to occur, our artificial neurons wire to the next layer sparsely, meaning that not all neurons from a layer are addressed by any neuron of the previous layer. We also take care to avoid having a neuron from one layer to target the same neuron of the next layer multiple times.

**\[0007\]** Once we have obtained our Current Input, we conjoin this to the end of the Input Queue and dequeue the oldest input from the beginning of the Input Queue. This is our short-term memory. We now seek to obtain an Output Unit. The Output Unit consists of an Action Sequence and a Parameter Sequence. After the Output Unit is obtained and any actions are performed, we address motivation with the Achieve State being recorded if a reward is available and with the Avoid State being recorded if a disincentive is available.

**\[0008\]** The Knowledge Bank format appears as follows, with the bits representing a given Action Sequence conjoined to the given Parameter Sequence to form a composite unit:  
Input State A \- Action Sequence 0 \-\> Location (Input State) X  
Input State A \- Action Sequence 1 \-\> Location (Input State) Y  
Input State A \- Action Sequence 2 \-\> Location (Input State) Z  
Locations X, Y, and Z contain the inputs reached when Action Sequences 0, 1, and 2 were performed after Input State A, respectively.

**\[0009\]** This is a logical implication in the form of (JK-\>P) && (JL-\>Q) && (JM-\>R), which can be represented in 2CNF (conjunctive normal form) (\~P | JK) && (\~Q | JL) && (\~R | JM). A 2SAT instance can be solved in linear time and a 3SAT instance can be solved quickly. If you elect to use the 3SAT approach, simply pad each 2CNF clause with an additional false value to force them to have exactly 3 literals. If an action sequence is found that leads to an Achieve State, the next action to perform is the first action branching from the current state (current location in the Knowledge Bank) to the next state in the sequence eventually leading to an Achieve State. We repeat for all Achieve State(s) in the system and we are left with action/step pairs. We repeat for all Avoid State(s) in the system and we are again left with action/step pairs.

**\[0010\]** To select the most appropriate action, we find action matches between the Achieve action/step pairs and the Avoid action/step pairs where the number of steps in the Avoid action/step pair is less than or equal to the number of steps in the matching Achieve action/step pairs. We eliminate these actions as options. We also eliminate all other Avoid actions from action/step pairs that don’t match any actions from the Achieve action/step pairs. If two or more Achieve States remain and are available from the current state, the one with the fewest intermediate states is selected. If there are multiple sequences leading to an Achieve State that are all tied for the fewest intermediate states, one is chosen at random. If no Achieve State action/step pairs are available, an action to perform is chosen at random from the remaining actions that have not been eliminated as possibilities. If no actions remain available, we select the action/step pair that results in the greatest number of steps before an Avoid State is reached. The output from this 2CNF solver portion of the cycle is the Output Unit, which consists of the Output Recall Bit, the Action Sequence, and the Parameter Sequence.

**\[0011\]** One of the output bits is a recall bit where the AGI reads from its knowledge bank based on the portion of the output unit contained in the rest of the output unit (the Utility Sequence). For the AGI to perform tasks such as controlling robotics or outputting to a display terminal, there will be a portion of the Output Unit that contains the Utility Sequence. This Utility Sequence is then parsed so that the remainder of the Output Unit received from the 2CNF logic solver- the Parameter Sequence- is sent to the correct Output Target as identified by the Action Sequence from the solution of the 2CNF instance from the prior step of the cycle. The recall action accepts the Action Sequence and the Parameter Sequence from the Output Unit and uses that to address a location in the Knowledge Bank. The data from this location is fetched and sent into the input as the new Current Input. It is worth noting that the Knowledge Bank is updated with every single cycle with a location within the Knowledge Bank vectoring with an action to another location, possibly a new location, to reflect a dynamic world. If the recall bit is not set in the Output Unit, then the Parameter Sequence is sent to the Output Target addressed by the Action Sequence, and the action is performed according to the Parameter Sequence, and input comes from the current sensory source to begin the next cycle.
